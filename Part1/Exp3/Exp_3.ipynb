{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.integrate import odeint\n",
    "import pdb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from math import sqrt\n",
    "from tensorflow.keras import layers, models\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pYcNGO-uK2J"
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_samples_sim(data, ph, hist, day_len_f):\n",
    "    # import pdb\n",
    "    \"\"\"\n",
    "    Create samples consisting in glucose, insulin and CHO histories (past hist-length values)\n",
    "    :param data: dataframe\n",
    "    :param ph: prediction horizon in minutes in sampling frequency scale\n",
    "    :param hist: history length in sampling frequency scale\n",
    "    :param day_len_f: length of day in sampling frequency scale\n",
    "    :return: dataframe of samples\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0] - ph - hist + 1\n",
    "    # pdb.set_trace()\n",
    "    y = data.loc[ph + hist - 1:, \"glucose\"].values.reshape(-1, 1)\n",
    "    d = pd.DatetimeIndex(data.loc[ph + hist - 1:, \"datetime\"].values)\n",
    "    # t = np.concatenate([np.arange(day_len_f) for _ in range(len(data) // day_len_f)], axis=0)[ph + hist - 1:].reshape(-1, 1)\n",
    "    g = np.array([data.loc[i:i + n_samples - 1, \"glucose\"] for i in range(hist)]).transpose()\n",
    "    c = np.array([data.loc[i:i + n_samples - 1, \"CHO\"] for i in range(hist)]).transpose()\n",
    "    i = np.array([data.loc[i:i + n_samples - 1, \"insulin\"] for i in range(hist)]).transpose()\n",
    "\n",
    "    new_columns = np.r_[[\"glucose_\" + str(i) for i in range(hist)], [\"CHO_\" + str(i) for i in range(hist)], [\n",
    "        \"insulin_\" + str(i) for i in range(hist)], [\"y\"]]\n",
    "    new_data = pd.DataFrame(data=np.c_[g, c, i, y], columns=new_columns)\n",
    "    new_data[\"datetime\"] = d\n",
    "    new_data = new_data.loc[:, np.r_[[\"datetime\"], new_columns]]  # reorder the columns, with datetime first\n",
    "\n",
    "    return new_data\n",
    "\n",
    "def standardize_sim(data):\n",
    "  columns = data.columns.drop([\"datetime\", 'y'])\n",
    "  scaler = StandardScaler()\n",
    "\n",
    "  # standardize the sets (-> ndarray) without datetime\n",
    "  train = scaler.fit_transform(data.drop([\"datetime\", 'y'], axis=1))\n",
    "\n",
    "  # recreate dataframe\n",
    "  train = pd.DataFrame(data=train, columns=columns)\n",
    "\n",
    "  # add datetime\n",
    "  train[\"datetime\"] = pd.DatetimeIndex(data.loc[:, \"datetime\"].values)\n",
    "  train[\"y\"] = data.loc[:, \"y\"].values\n",
    "\n",
    "  # reorder\n",
    "  train = train.loc[:, data.columns]\n",
    "\n",
    "  train = train.reset_index(drop=True)\n",
    "\n",
    "  return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2ftdMh8tSc1"
   },
   "source": [
    "###Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj9qW39wH6co"
   },
   "outputs": [],
   "source": [
    "seed = 1  \n",
    "cv = 5 \n",
    "path = \".\" \n",
    "freq = 5\n",
    "day_len = 1440\n",
    "day_len_freq = day_len // freq\n",
    "n_days_test = 10\n",
    "\n",
    "params = {\n",
    "    \"hist\": 180\n",
    "}\n",
    "\n",
    "search = {\n",
    "    \"lr\": [\"logarithmic\", 3, 3],\n",
    "}\n",
    "\n",
    "hist = params[\"hist\"] // freq\n",
    "\n",
    "\n",
    "dataset='ohio'\n",
    "subject='575'\n",
    "ph=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxPHa3Me9Dlx"
   },
   "outputs": [],
   "source": [
    "def aggregate_inputs(data1):\n",
    "    \"\"\"\n",
    "    Aggregate historical values to prepare inputs for the Hovorka model.\n",
    "    :param data: preprocessed dataframe with 36 columns each for glucose, CHO, and insulin\n",
    "    :return: dataframe with aggregated inputs for the Hovorka model\n",
    "    \"\"\"\n",
    "    data = data1.copy()\n",
    "    # List of historical columns\n",
    "    glucose_cols = [f'glucose_{i}' for i in range(36)]\n",
    "    cho_cols = [f'CHO_{i}' for i in range(36)]\n",
    "    insulin_cols = [f'insulin_{i}' for i in range(36)]\n",
    "\n",
    "    # Calculate the aggregated inputs\n",
    "    data['glucose_input'] = data[glucose_cols].mean(axis=1)\n",
    "    data['CHO_input'] = data[cho_cols].sum(axis=1)\n",
    "    data['insulin_input'] = data[insulin_cols].sum(axis=1)\n",
    "\n",
    "    # Select the relevant columns for the Hovorka model\n",
    "    hovorka_inputs = data[['datetime', 'glucose_input', 'CHO_input', 'insulin_input', 'y']]\n",
    "\n",
    "    return hovorka_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_range(data1):\n",
    "    current_min = data1['CGM'].min()\n",
    "    current_max = data1['CGM'].max()\n",
    "    new_min = 39\n",
    "    new_max = 450\n",
    "    # Apply the linear transformation to scale the CGM values\n",
    "    data1['CGM_scaled'] = ((data1['CGM'] - current_min) / (current_max - current_min)) * (new_max - new_min) + new_min\n",
    "    data1.drop(columns=['CGM'], inplace=True)\n",
    "    return data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eFCru-Iyyzh"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/Users/narayaniverma/Documents/Masters_Dissertation/UVA/simglucose-master/results/results_23Aug_Final_1440/adult#009.csv', usecols = ['Time', 'CGM', 'CHO', 'insulin'])\n",
    "data.drop(data.tail(1).index, inplace = True)\n",
    "data = scaling_range(data)\n",
    "data.rename(columns={'Time': 'datetime', 'CGM_scaled': 'glucose', 'CHO': 'CHO', 'insulin': 'insulin'}, inplace=True)\n",
    "\n",
    "data_sampled_sim = create_samples_sim(data, ph, hist, day_len_freq)\n",
    "data_hovorka_inputs_sim = aggregate_inputs(data_sampled_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPbvpsAb5s58"
   },
   "outputs": [],
   "source": [
    "G0 = 5\n",
    "Bow = 70\n",
    "k12 = 0.066\n",
    "ka1 = 0.006\n",
    "ka2 = 0.06\n",
    "ka3 = 0.03\n",
    "kb1 = 3.072 * 10**-5\n",
    "kb2 = 4.92 * 10**-5\n",
    "kb3 = 0.00156\n",
    "ke = 0.138\n",
    "VI = 0.12 * Bow\n",
    "VG = 0.16 * Bow\n",
    "AG = 0.8\n",
    "tmax_G = 40\n",
    "tmax_I = 55\n",
    "SFIT = 51.2 * 10**-4\n",
    "SFID = 8.2 * 10**-4\n",
    "SFIE = 520 * 10**-4\n",
    "EGP0 = 0.0161 * Bow\n",
    "F01 = 0.0097 * Bow\n",
    "Q10 = G0 * VG\n",
    "u0 = 6.68\n",
    "S10 = u0 * tmax_I\n",
    "S20 = S10\n",
    "Ui0 = S20/tmax_I\n",
    "I0 = Ui0/(ke * VI)\n",
    "x10 = SFIT * I0\n",
    "x20 = SFID * I0\n",
    "x30 = SFIE * I0\n",
    "Q20 = x10 / (x20 + k12)\n",
    "\n",
    "preprocessed_data_w_hovorka_sim = []\n",
    "\n",
    "\n",
    "# Function to define Hovorka model equations\n",
    "def hovorka_model(y, t, d, u):\n",
    "    Q1, Q2, x1, x2, x3, D1, D2, S1, S2, I = y\n",
    "\n",
    "    # Glucose absorption compartments\n",
    "    D = 1000 * 180.156 * np.interp(t, np.arange(len(d)), d)\n",
    "    dD1_dt = AG * D - D1 / tmax_G\n",
    "    dD2_dt = D1 / tmax_G - D2 / tmax_G\n",
    "    UG = D2 / tmax_G\n",
    "\n",
    "    # Amount of glucose\n",
    "    F01_c = F01 if Q1 / VG >= 4.5 else F01 * (Q1 / VG / 4.5)\n",
    "    FR = 0.003 * (Q1 / VG - 9) * VG if Q1 / VG >= 9 else 0\n",
    "\n",
    "    # Amount of glucose\n",
    "    dQ1_dt = UG - x1 * Q1 - F01_c - FR + k12 * Q2 + EGP0 * (1 - x3)\n",
    "    dQ2_dt = x1 * Q1 - (k12 + x2) * Q2\n",
    "    G = Q1 / VG\n",
    "\n",
    "    # Insulin effects\n",
    "    dx1_dt = -ka1 * x1 + kb1 * I\n",
    "    dx2_dt = -ka2 * x2 + kb2 * I\n",
    "    dx3_dt = -ka3 * x3 + kb3 * I\n",
    "\n",
    "    # Insulin absorption compartments\n",
    "    dS1_dt = np.interp(t, np.arange(len(u)), u) - S1 / tmax_I\n",
    "    dS2_dt = S1 / tmax_I - S2 / tmax_I\n",
    "    dI_dt = S2 / tmax_I / VI - ke * I\n",
    "\n",
    "    return [dQ1_dt, dQ2_dt, dx1_dt, dx2_dt, dx3_dt, dD1_dt, dD2_dt, dS1_dt, dS2_dt, dI_dt]\n",
    "\n",
    "for k,j in [(data_hovorka_inputs_sim, data_sampled_sim)]:\n",
    "  glucose_avg = k['glucose_input'].values\n",
    "  CHO_sum = k['CHO_input'].values\n",
    "  insulin_sum = k['insulin_input'].values\n",
    "\n",
    "  # Define the time points (assuming they are evenly spaced)\n",
    "  time_points = np.arange(len(k))\n",
    "  initial_conditions = [Q10, Q20, x10, x20, x30, 0, 0, S10, S20, I0]\n",
    "\n",
    "  # Integrate the Hovorka model equations over time\n",
    "  results = odeint(hovorka_model, initial_conditions, time_points, args=(CHO_sum, insulin_sum))\n",
    "\n",
    "  # Extracting features\n",
    "  G_t = results[:, 0] / VG\n",
    "  I_t = results[:, -1]\n",
    "  UG_t = results[:, 6] / tmax_G\n",
    "\n",
    "  # Create a dataframe with the results\n",
    "  hovorka_features = pd.DataFrame({\n",
    "      'G_t': G_t,\n",
    "      'I_t': I_t,\n",
    "      'UG_t': UG_t\n",
    "  }, index=j.index)\n",
    "\n",
    "  preprocessed_data_w_hovorka_sim = pd.concat([j, hovorka_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "okVi1YEc60Ej"
   },
   "outputs": [],
   "source": [
    "k = preprocessed_data_w_hovorka_sim\n",
    "l = data_sampled_sim\n",
    "l_g = pd.DataFrame()\n",
    "l_c = pd.DataFrame()\n",
    "l_i = pd.DataFrame()\n",
    "l_y = pd.DataFrame()\n",
    "\n",
    "n_samples = k.shape[0] - ph - hist + 1\n",
    "\n",
    "g = np.array([k.loc[i:i + n_samples - 1, \"G_t\"] for i in range(hist)]).transpose()\n",
    "i_n = np.array([k.loc[i:i + n_samples - 1, \"I_t\"] for i in range(hist)]).transpose()\n",
    "c = np.array([k.loc[i:i + n_samples - 1, \"UG_t\"] for i in range(hist)]).transpose()\n",
    "\n",
    "new_columns = np.r_[[\"G_t\" + str(i) for i in range(hist)], [\"I_t\" + str(i) for i in range(hist)], [\n",
    "      \"UG_t\" + str(i) for i in range(hist)]]\n",
    "new_data = pd.DataFrame(data=np.c_[g, i_n, c], columns=new_columns)\n",
    "# Assuming 'train' is your list of DataFrames with original sequential data\n",
    "# Merge Hovorka sequences with each DataFrame in train\n",
    "\n",
    "l_g = l.loc[:, :'glucose_35']\n",
    "l_c = l.loc[:, 'CHO_0':'CHO_35']\n",
    "l_i = l.loc[:, 'insulin_0':'insulin_35']\n",
    "l_y = l.loc[:, 'y']\n",
    "new_data_g = new_data.loc[:, 'G_t0':'G_t35']\n",
    "new_data_c = new_data.loc[:, 'UG_t0':'UG_t35']\n",
    "new_data_i = new_data.loc[:, 'I_t0':'I_t35']\n",
    "l = pd.concat([l_g, new_data_g.iloc[:len(l)].reset_index(drop=True), l_c, new_data_c.iloc[:len(l)].reset_index(drop=True), l_i, new_data_i.iloc[:len(l)].reset_index(drop=True), l_y], axis=1)\n",
    "list_of_cols = l.loc[:0, 'G_t0':'UG_t35'].columns  # retrieve only the 0th row for efficiency\n",
    "l.dropna(subset = list_of_cols, inplace = True)\n",
    "\n",
    "data_sampled_sim = l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h2sqk7E7BFfV"
   },
   "outputs": [],
   "source": [
    "train = standardize_sim(data_sampled_sim)\n",
    "\n",
    "X_train_sim = train.drop(['y', 'datetime'], axis=1).values\n",
    "y_train_sim = train['y'].values\n",
    "\n",
    "X_train_sim = X_train_sim.reshape(-1, 36, 6)\n",
    "y_train_sim = y_train_sim.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeJ48VwP0aZK"
   },
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    PE = np.zeros((seq_len, d_model))\n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            PE[pos, i] = np.sin(pos / (10000 ** ((2 * i) / d_model)))\n",
    "            if i + 1 < d_model:\n",
    "                PE[pos, i + 1] = np.cos(pos / (10000 ** ((2 * (i + 1)) / d_model)))\n",
    "    return tf.constant(PE, dtype=tf.float32)\n",
    "\n",
    "# d_model = len(features)  # This should match the number of input features\n",
    "pos_encoding = positional_encoding(36, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "myu2s6pi6aG2",
    "outputId": "3fa2376d-c764-48f4-f3b5-6b81e75e14b2"
   },
   "outputs": [],
   "source": [
    "def transformer_model(input_shape, d_model, num_heads, num_layers, dropout=0.1):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = inputs + pos_encoding[:input_shape[0], :]\n",
    "\n",
    "    for _ in range(num_layers):\n",
    "        x = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)\n",
    "        x = layers.Dropout(dropout)(x)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "\n",
    "        x_ff = layers.Dense(d_model, activation='relu')(x)\n",
    "        x_ff = layers.Dropout(dropout)(x_ff)\n",
    "        x = layers.LayerNormalization(epsilon=1e-6)(x_ff + x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (36, 6)\n",
    "d_model = 6\n",
    "num_heads = 8\n",
    "num_layers = 3\n",
    "dropout = 0.1\n",
    "\n",
    "model = transformer_model(input_shape, d_model, num_heads, num_layers, dropout)\n",
    "model.compile(optimizer='adam', loss='mse',\n",
    "              metrics=[tf.keras.metrics.RootMeanSquaredError(), 'mape'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='loss',  # you can also use 'val_rmse' or any other metric you're monitoring\n",
    "                               patience=10,         # number of epochs with no improvement after which training will be stopped\n",
    "                               restore_best_weights=True,  # restores model weights from the epoch with the best value of the monitored metric\n",
    "                               verbose=1)\n",
    "\n",
    "early_stopping_real = EarlyStopping(monitor='loss',\n",
    "                                    patience=10,\n",
    "                                    restore_best_weights=True,\n",
    "                                    verbose=1)\n",
    "\n",
    "# Define ReduceLROnPlateau callback\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss',   \n",
    "                              factor=0.5,           # factor by which the learning rate will be reduced\n",
    "                              patience=5,           # number of epochs with no improvement after which learning rate will be reduced\n",
    "                              min_lr=1e-6,          # lower bound on the learning rate\n",
    "                              verbose=1)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "14Kc-68l_JNJ",
    "outputId": "f7d0fddd-68b0-4ecc-eb47-cf49c822d253"
   },
   "outputs": [],
   "source": [
    "history_sim = model.fit(X_train_sim, y_train_sim, epochs=20, batch_size=32, callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLPvF0jjAGls"
   },
   "outputs": [],
   "source": [
    "model.save_weights('model_sim.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jHVyRDh73l4N"
   },
   "source": [
    "##Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XmYI0B3R3YxZ"
   },
   "source": [
    "###Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "glONq8g53XFU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def fill_nans(data, day_len, n_days_test):\n",
    "    \"\"\"\n",
    "    Fill NaNs inside the dataframe of samples following:\n",
    "    - CHO and insulin values are filled with 0\n",
    "    - glucose history are interpolated (linearly) when possible and extrapolated if not\n",
    "    :param data: sample dataframe\n",
    "    :param day_len: length of day, scaled to sampling frequency\n",
    "    :param n_days_test: number of test days\n",
    "    :return: cleaned sample dataframe\n",
    "    \"\"\"\n",
    "    data_nan = data.copy()\n",
    "\n",
    "    # fill insulin and CHO nans with 0\n",
    "    for col in data.columns:\n",
    "        if \"insulin\" in col or \"CHO\" in col:\n",
    "            data[col] = data[col].fillna(0)\n",
    "\n",
    "    # fill glucose nans\n",
    "    g_col = [col for col in data.columns if \"glucose\" in col]\n",
    "    g_mean = data.y.mean()\n",
    "    for i in range(len(data.index)):\n",
    "        g_i = data.loc[i, g_col]\n",
    "        if g_i.notna().all():  # no nan\n",
    "            pass\n",
    "        elif g_i.isna().all():  # all nan\n",
    "            if i > len(data) - n_days_test * day_len + 1:\n",
    "                last_known_gs = data_nan.loc[:i - 1, \"glucose_0\"][data_nan.loc[:i - 1, \"glucose_0\"].notna()]\n",
    "                if len(last_known_gs) >= 1:\n",
    "                    for col in g_col:\n",
    "                        # data.loc[i,col] = last_known_gs.iloc[-1]\n",
    "                        data.loc[i, col] = g_mean\n",
    "                else:\n",
    "                    for col in g_col:\n",
    "                        data.loc[i, col] = g_mean\n",
    "        else:  # some nan\n",
    "            # compute insample nan indices, and last outsample + insample nonnan indices\n",
    "            isna_idx = i + np.where(g_i.isna())[0]\n",
    "            notna_idx = i + np.where(g_i.notna())[0]\n",
    "            if data_nan.loc[:i - 1, \"glucose_0\"].notna().any():\n",
    "                mask = data_nan.loc[:i - 1, \"glucose_0\"].notna().values\n",
    "                notna_idx = np.r_[data_nan.loc[:i - 1, \"glucose_0\"][mask].index[-2:], notna_idx]\n",
    "\n",
    "            # for all nan\n",
    "            for isna_i in isna_idx:\n",
    "                # get the two closest non nan values\n",
    "                idx_diff = notna_idx - isna_i\n",
    "\n",
    "                if np.any(idx_diff > 0) and np.any(idx_diff < 0):\n",
    "                    # we got a start and an end\n",
    "                    start = notna_idx[np.where(idx_diff < 0, idx_diff, -np.inf).argmax()]\n",
    "                    end = notna_idx[np.where(idx_diff > 0, idx_diff, np.inf).argmin()]\n",
    "                    start_val, end_val = data_nan.loc[start, \"glucose_0\"], data_nan.loc[end, \"glucose_0\"]\n",
    "\n",
    "                    # interpolate between them\n",
    "                    rate = (end_val - start_val) / (end - start)\n",
    "                    data.loc[i, g_col[isna_i - i]] = data_nan.loc[start, \"glucose_0\"] + rate * (isna_i - start)\n",
    "                elif np.any(idx_diff > 0):\n",
    "                    # we only have end(s)\n",
    "                    # backward extrapolation - only used in very first day where there is no start\n",
    "                    if len(idx_diff) >= 2:\n",
    "                        # we have two last values so we can compute a rate\n",
    "                        end1, end2 = notna_idx[0], notna_idx[1]\n",
    "                        end1_val, end2_val = data_nan.loc[end1, \"glucose_0\"], data_nan.loc[end2, \"glucose_0\"]\n",
    "                        rate = (end2_val - end1_val) / (end2 - end1)\n",
    "                        data.loc[i, g_col[isna_i - i]] = data_nan.loc[end1, \"glucose_0\"] - rate * (end1 - isna_i)\n",
    "                    else:\n",
    "                        # we have only one value so we cannot compute a rate\n",
    "                        end = notna_idx[0]\n",
    "                        end_val = data_nan.loc[end, \"glucose_0\"]\n",
    "                        data.loc[i, g_col[isna_i - i]] = end_val\n",
    "                elif np.any(idx_diff < 0):\n",
    "                    last_val = g_i[g_i.notna()][-1]\n",
    "                    data.loc[i, g_col[isna_i - i]] = last_val\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def remove_nans(data):\n",
    "    \"\"\"\n",
    "    Remove samples that still have NaNs (in y column mostly)\n",
    "    :param data: dataframe of samples\n",
    "    :return: no-NaN dataframe\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for df in data:\n",
    "        new_data.append(df.dropna())\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8grRUzQ3iOT"
   },
   "source": [
    "###Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rQZqSvK2gSV"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_ohio(dataset, subject):\n",
    "    \"\"\"\n",
    "    Load OhioT1DM training_old and testing files into a dataframe\n",
    "    :param dataset: name of dataset\n",
    "    :param subject: name of subject\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "    train_path, test_path = _compute_file_names(dataset, subject)\n",
    "\n",
    "    [train_xml, test_xml] = [ET.parse(set).getroot() for set in [train_path, test_path]]\n",
    "\n",
    "    [train, test] = [_extract_data_from_xml(xml) for xml in [train_xml, test_xml]]\n",
    "\n",
    "    data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _compute_file_names(dataset, subject):\n",
    "    \"\"\"\n",
    "    Compute the name of the files, given dataset and subject\n",
    "    :param dataset: name of dataset\n",
    "    :param subject: name of subject\n",
    "    :return: path to training file, path to testing file\n",
    "    \"\"\"\n",
    "    train_dir = \"/Users/narayaniverma/Documents/Masters_Dissertation/OhioT1DM/2018/train\"\n",
    "    train_file = subject + \"-ws-training.xml\"\n",
    "    train_path = os.path.join(train_dir, train_file)\n",
    "\n",
    "    test_dir = \"/Users/narayaniverma/Documents/Masters_Dissertation/OhioT1DM/2018/test\"\n",
    "    test_file = subject + \"-ws-testing.xml\"\n",
    "    test_path = os.path.join(test_dir, test_file)\n",
    "\n",
    "    return train_path, test_path\n",
    "\n",
    "\n",
    "def _extract_data_from_xml(xml):\n",
    "    \"\"\"\n",
    "    extract glucose, CHO, and insulin from xml and merge the data\n",
    "    :param xml:\n",
    "    :return: dataframe\n",
    "    \"\"\"\n",
    "    glucose_df = _get_glucose_from_xml(xml)\n",
    "    CHO_df = _get_CHO_from_xml(xml)\n",
    "    insulin_df = _get_insulin_from_xml(xml)\n",
    "\n",
    "    print(\"glocose:\", glucose_df)\n",
    "    print(\"CHO:\", CHO_df)\n",
    "    print(\"insulin:\", insulin_df)\n",
    "\n",
    "    df = pd.merge(glucose_df, CHO_df, how=\"outer\", on=\"datetime\")\n",
    "    df = pd.merge(df, insulin_df, how=\"outer\", on=\"datetime\")\n",
    "    df = df.sort_values(\"datetime\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _get_field_labels(etree, field_index):\n",
    "    \"\"\"\n",
    "    extract labels from xml tree\n",
    "    :param etree: etree\n",
    "    :param field_index:  position of field\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"list of field labels:\", list(etree[field_index][0].attrib.keys()))\n",
    "    return list(etree[field_index][0].attrib.keys())\n",
    "\n",
    "\n",
    "def _iter_fields(etree, field_index):\n",
    "    \"\"\"\n",
    "    extract columns inside xml tree\n",
    "    :param etree: tree\n",
    "    :param field_index: position of columns\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for event in etree[field_index].iter(\"event\"):\n",
    "        yield list(event.attrib.values())\n",
    "\n",
    "\n",
    "def _get_CHO_from_xml(xml):\n",
    "    \"\"\"\n",
    "    Extract CHO values from xml\n",
    "    :param xml:\n",
    "    :return: CHO dataframe\n",
    "    \"\"\"\n",
    "    labels = _get_field_labels(xml, field_index=5)\n",
    "    CHO = list(_iter_fields(xml, field_index=5))\n",
    "    CHO_df = pd.DataFrame(data=CHO, columns=labels)\n",
    "    CHO_df.drop(\"type\", axis=1, inplace=True)\n",
    "    CHO_df[\"ts\"] = pd.to_datetime(CHO_df[\"ts\"], format=\"%d-%m-%Y %H:%M:%S\")\n",
    "    CHO_df[\"carbs\"] = CHO_df[\"carbs\"].astype(\"float\")\n",
    "    CHO_df.rename(columns={'ts': 'datetime', 'carbs': 'CHO'}, inplace=True)\n",
    "    return CHO_df\n",
    "\n",
    "\n",
    "def _get_insulin_from_xml(xml):\n",
    "    \"\"\"\n",
    "    Extract insulin values from xml\n",
    "    :param xml:\n",
    "    :return: insulin dataframe\n",
    "    \"\"\"\n",
    "    labels = _get_field_labels(xml, field_index=4)\n",
    "    insulin = list(_iter_fields(xml, field_index=4))\n",
    "    insulin_df = pd.DataFrame(data=insulin, columns=labels)\n",
    "    for col in [\"ts_end\", \"type\", \"bwz_carb_input\"]:\n",
    "        insulin_df.drop(col, axis=1, inplace=True)\n",
    "    insulin_df[\"ts_begin\"] = pd.to_datetime(insulin_df[\"ts_begin\"], format=\"%d-%m-%Y %H:%M:%S\")\n",
    "    insulin_df[\"dose\"] = insulin_df[\"dose\"].astype(\"float\")\n",
    "    insulin_df.rename(columns={'ts_begin': 'datetime', 'dose': 'insulin'}, inplace=True)\n",
    "    return insulin_df\n",
    "\n",
    "\n",
    "def _get_glucose_from_xml(xml):\n",
    "    \"\"\"\n",
    "    Extract glucose values from xml\n",
    "    :param xml:\n",
    "    :return: glucose dataframe\n",
    "    \"\"\"\n",
    "    labels = _get_field_labels(xml, field_index=0)\n",
    "    glucose = list(_iter_fields(xml, field_index=0))\n",
    "    glucose_df = pd.DataFrame(data=glucose, columns=labels)\n",
    "    glucose_df[\"ts\"] = pd.to_datetime(glucose_df[\"ts\"], format=\"%d-%m-%Y %H:%M:%S\")\n",
    "    glucose_df[\"value\"] = glucose_df[\"value\"].astype(\"float\")\n",
    "    glucose_df.rename(columns={'ts': 'datetime', 'value': 'glucose'}, inplace=True)\n",
    "    return glucose_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VcQ0n4Sv3saL"
   },
   "source": [
    "###Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hv3MBROE2vYc"
   },
   "outputs": [],
   "source": [
    "from datetime import timedelta, datetime\n",
    "def resample(data, freq):\n",
    "    \"\"\"\n",
    "    :param data: dataframe\n",
    "    :param freq: sampling frequency\n",
    "    :return: resampled data between the the first day at 00:00:00 and the last day at 23:60-freq:00 at freq sample frequency\n",
    "    \"\"\"\n",
    "    start = data.datetime.iloc[0].strftime('%Y-%m-%d') + \" 00:00:00\"\n",
    "    end = datetime.strptime(data.datetime.iloc[-1].strftime('%Y-%m-%d'), \"%Y-%m-%d\") + timedelta(days=1) - timedelta(\n",
    "        minutes=freq)\n",
    "    index = pd.period_range(start=start,\n",
    "                            end=end,\n",
    "                            freq=str(freq) + 'min').to_timestamp()\n",
    "    data = data.resample(str(freq) + 'min', on=\"datetime\").agg({'glucose': np.mean, 'CHO': np.sum, \"insulin\": np.sum})\n",
    "    data = data.reindex(index=index)\n",
    "    data = data.reset_index()\n",
    "    data = data.rename(columns={\"index\": \"datetime\"})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpGNpTD23x4Z"
   },
   "source": [
    "###Create Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRLeqgWKF0nl"
   },
   "outputs": [],
   "source": [
    "def create_samples(data, ph, hist, day_len_f):\n",
    "    \"\"\"\n",
    "    Create samples consisting in glucose, insulin and CHO histories (past hist-length values)\n",
    "    :param data: dataframe\n",
    "    :param ph: prediction horizon in minutes in sampling frequency scale\n",
    "    :param hist: history length in sampling frequency scale\n",
    "    :param day_len_f: length of day in sampling frequency scale\n",
    "    :return: dataframe of samples\n",
    "    \"\"\"\n",
    "    n_samples = data.shape[0] - ph - hist + 1\n",
    "\n",
    "    y = data.loc[ph + hist - 1:, \"glucose\"].values.reshape(-1, 1)\n",
    "    d = pd.DatetimeIndex(data.loc[ph + hist - 1:, \"datetime\"].values)\n",
    "    t = np.concatenate([np.arange(day_len_f) for _ in range(len(data) // day_len_f)], axis=0)[ph + hist - 1:].reshape(-1, 1)\n",
    "    g = np.array([data.loc[i:i + n_samples - 1, \"glucose\"] for i in range(hist)]).transpose()\n",
    "    c = np.array([data.loc[i:i + n_samples - 1, \"CHO\"] for i in range(hist)]).transpose()\n",
    "    i = np.array([data.loc[i:i + n_samples - 1, \"insulin\"] for i in range(hist)]).transpose()\n",
    "\n",
    "    new_columns = np.r_[[\"time\"], [\"glucose_\" + str(i) for i in range(hist)], [\"CHO_\" + str(i) for i in range(hist)], [\n",
    "        \"insulin_\" + str(i) for i in range(hist)], [\"y\"]]\n",
    "    new_data = pd.DataFrame(data=np.c_[t, g, c, i, y], columns=new_columns)\n",
    "    new_data[\"datetime\"] = d\n",
    "    new_data = new_data.loc[:, np.r_[[\"datetime\"], new_columns]]  # reorder the columns, with datetime first\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXqOw6WqGb-9"
   },
   "source": [
    "###Remove/Fill Nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LjkNlVPF4q-"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_nans(data):\n",
    "    \"\"\"\n",
    "    Remove samples that still have NaNs (in y column mostly)\n",
    "    :param data: dataframe of samples\n",
    "    :return: no-NaN dataframe\n",
    "    \"\"\"\n",
    "    new_data = []\n",
    "    for df in data:\n",
    "        new_data.append(df.dropna())\n",
    "    return new_data\n",
    "\n",
    "def fill_nans(data, day_len, n_days_test):\n",
    "    \"\"\"\n",
    "    Fill NaNs inside the dataframe of samples following:\n",
    "    - CHO and insulin values are filled with 0\n",
    "    - glucose history are interpolated (linearly) when possible and extrapolated if not\n",
    "    :param data: sample dataframe\n",
    "    :param day_len: length of day, scaled to sampling frequency\n",
    "    :param n_days_test: number of test days\n",
    "    :return: cleaned sample dataframe\n",
    "    \"\"\"\n",
    "    data_nan = data.copy()\n",
    "\n",
    "    # fill insulin and CHO nans with 0\n",
    "    for col in data.columns:\n",
    "        if \"insulin\" in col or \"CHO\" in col:\n",
    "            data[col] = data[col].fillna(0)\n",
    "\n",
    "    # fill glucose nans\n",
    "    g_col = [col for col in data.columns if \"glucose\" in col]\n",
    "    g_mean = data.y.mean()\n",
    "    for i in range(len(data.index)):\n",
    "        g_i = data.loc[i, g_col]\n",
    "        if g_i.notna().all():  # no nan\n",
    "            pass\n",
    "        elif g_i.isna().all():  # all nan\n",
    "            if i > len(data) - n_days_test * day_len + 1:\n",
    "                last_known_gs = data_nan.loc[:i - 1, \"glucose_0\"][data_nan.loc[:i - 1, \"glucose_0\"].notna()]\n",
    "                if len(last_known_gs) >= 1:\n",
    "                    for col in g_col:\n",
    "                        # data.loc[i,col] = last_known_gs.iloc[-1]\n",
    "                        data.loc[i, col] = g_mean\n",
    "                else:\n",
    "                    for col in g_col:\n",
    "                        data.loc[i, col] = g_mean\n",
    "        else:  # some nan\n",
    "            # compute insample nan indices, and last outsample + insample nonnan indices\n",
    "            isna_idx = i + np.where(g_i.isna())[0]\n",
    "            notna_idx = i + np.where(g_i.notna())[0]\n",
    "            if data_nan.loc[:i - 1, \"glucose_0\"].notna().any():\n",
    "                mask = data_nan.loc[:i - 1, \"glucose_0\"].notna().values\n",
    "                notna_idx = np.r_[data_nan.loc[:i - 1, \"glucose_0\"][mask].index[-2:], notna_idx]\n",
    "\n",
    "            # for all nan\n",
    "            for isna_i in isna_idx:\n",
    "                # get the two closest non nan values\n",
    "                idx_diff = notna_idx - isna_i\n",
    "\n",
    "                if np.any(idx_diff > 0) and np.any(idx_diff < 0):\n",
    "                    # we got a start and an end\n",
    "                    start = notna_idx[np.where(idx_diff < 0, idx_diff, -np.inf).argmax()]\n",
    "                    end = notna_idx[np.where(idx_diff > 0, idx_diff, np.inf).argmin()]\n",
    "\n",
    "                    start_idx = _compute_indexes(i, start, len(data_nan))\n",
    "                    end_idx = _compute_indexes(i, end, len(data_nan))\n",
    "\n",
    "                    start_val = data_nan.loc[start_idx]\n",
    "                    end_val = data_nan.loc[end_idx]\n",
    "\n",
    "                    # interpolate between them\n",
    "                    rate = (end_val - start_val) / (end - start)\n",
    "                    data.loc[i, g_col[isna_i - i]] = data_nan.loc[start_idx] + rate * (isna_i - start)\n",
    "                elif np.any(idx_diff > 0):\n",
    "                    # we only have end(s)\n",
    "                    # backward extrapolation - only used in very first day where there is no start\n",
    "                    if len(idx_diff) >= 2:\n",
    "                        # we have two last values so we can compute a rate\n",
    "                        end1, end2 = notna_idx[0], notna_idx[1]\n",
    "                        [end1_idx, end2_idx] = [_compute_indexes(i, _, len(data_nan)) for _ in [end1, end2]]\n",
    "                        end1_val, end2_val = data_nan.loc[end1_idx], data_nan.loc[end2_idx]\n",
    "                        rate = (end2_val - end1_val) / (end2 - end1)\n",
    "                        data.loc[i, g_col[isna_i - i]] = data_nan.loc[end1_idx] - rate * (end1 - isna_i)\n",
    "                    else:\n",
    "                        # we have only one value so we cannot compute a rate\n",
    "                        end = notna_idx[0]\n",
    "                        end_idx = _compute_indexes(i, end, len(data_nan))\n",
    "                        end_val = data_nan.loc[end_idx]\n",
    "                        data.loc[i, g_col[isna_i - i]] = end_val\n",
    "                elif np.any(idx_diff < 0):\n",
    "                    # forward extrapolation\n",
    "                    if len(idx_diff) >= 2:\n",
    "                        end1, end2 = notna_idx[-2], notna_idx[-1]\n",
    "                        [end1_idx, end2_idx] = [_compute_indexes(i, _, len(data_nan)) for _ in [end1, end2]]\n",
    "                        end1_val, end2_val = data_nan.loc[end1_idx], data_nan.loc[end2_idx]\n",
    "                        rate = (end2_val - end1_val) / (end2 - end1)\n",
    "                        data.loc[i, g_col[isna_i - i]] = data_nan.loc[end1_idx] - rate * (end1 - isna_i)\n",
    "                    else:\n",
    "                        # we only have one value, so we cannot compute a rate\n",
    "                        last_val = g_i[g_i.notna()][-1]\n",
    "                        data.loc[i, g_col[isna_i - i]] = last_val\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def _compute_indexes(i, index, len):\n",
    "    if index >= len:\n",
    "        return (i, \"glucose_\" + str(index - i))\n",
    "    else:\n",
    "        return (index, \"glucose_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILALxoftGrEq"
   },
   "source": [
    "###Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66RDl5I_Gts9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.model_selection as sk_model_selection\n",
    "\n",
    "def split(data, day_len, test_n_days, cv_factor):\n",
    "    \"\"\"\n",
    "    Split samples into training, validation, and testing days. Testing days are the last test_n_days days, and training\n",
    "    and validation sets are created by permuting of splits according to the cv_factor.\n",
    "    :param data: dataframe of samples\n",
    "    :param day_len: length of day in freq minutes\n",
    "    :param test_n_days: number of testing days\n",
    "    :param cv_factor: cross validation factor\n",
    "    :return: training, validation, testing samples folds\n",
    "    \"\"\"\n",
    "    # the test split is equal to the last test_n_days days of data\n",
    "    test = [data.iloc[-test_n_days * day_len:].copy()]\n",
    "\n",
    "    # train+valid samples = all minus first and test days\n",
    "    fday_n_samples = data.shape[0] - (data.shape[0] // day_len * day_len)\n",
    "    train_valid = data.iloc[fday_n_samples:-test_n_days * day_len].copy()\n",
    "\n",
    "    # split train_valid into cv_factor folds for inner cross-validation\n",
    "    n_days = train_valid.shape[0] // day_len\n",
    "    days = np.arange(n_days)\n",
    "\n",
    "    kf = sk_model_selection.KFold(n_splits=cv_factor, shuffle=True, random_state=seed)\n",
    "\n",
    "    train, valid = [], []\n",
    "    for train_idx, valid_idx in kf.split(days):\n",
    "        def get_whole_day(data, i):\n",
    "            return data[i * day_len:(i + 1) * day_len]\n",
    "\n",
    "        train.append(pd.concat([get_whole_day(train_valid, i) for i in train_idx], axis=0, ignore_index=True))\n",
    "        valid.append(pd.concat([get_whole_day(train_valid, i) for i in valid_idx], axis=0, ignore_index=True))\n",
    "\n",
    "    return train, valid, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El3Rc5iNG5Zm"
   },
   "source": [
    "###Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z1Rm8ilG8B4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize(train, valid, test):\n",
    "    \"\"\"\n",
    "    Standardize (zero mean and unit variance) the sets w.r.t to the training set for every fold\n",
    "    :param train: training sample fold\n",
    "    :param valid: validation sample fold\n",
    "    :param test: testing sample fold\n",
    "    :return: standardized training, validation, and testing sets\n",
    "    \"\"\"\n",
    "    columns = train[0].columns.drop([\"datetime\", 'y'])\n",
    "    train_scaled, valid_scaled, test_scaled, scalers = [], [], [], []\n",
    "    for i in range(len(train)):\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # standardize the sets (-> ndarray) without datetime\n",
    "        train_i = scaler.fit_transform(train[i].drop([\"datetime\", 'y'], axis=1))\n",
    "        valid_i = scaler.transform(valid[i].drop([\"datetime\", 'y'], axis=1))\n",
    "        test_i = scaler.transform(test[0].copy().drop([\"datetime\", 'y'], axis=1))\n",
    "\n",
    "        # recreate dataframe\n",
    "        train_i = pd.DataFrame(data=train_i, columns=columns)\n",
    "        valid_i = pd.DataFrame(data=valid_i, columns=columns)\n",
    "        test_i = pd.DataFrame(data=test_i, columns=columns)\n",
    "\n",
    "        # add datetime\n",
    "        train_i[\"datetime\"] = pd.DatetimeIndex(train[i].loc[:, \"datetime\"].values)\n",
    "        train_i[\"y\"] = train[i].loc[:, \"y\"].values\n",
    "        valid_i[\"datetime\"] = pd.DatetimeIndex(valid[i].loc[:, \"datetime\"].values)\n",
    "        valid_i[\"y\"] = valid[i].loc[:, \"y\"].values\n",
    "        test_i[\"datetime\"] = pd.DatetimeIndex(test[0].loc[:, \"datetime\"].values)\n",
    "        test_i[\"y\"] = test[0].loc[:, \"y\"].values\n",
    "\n",
    "        # reorder\n",
    "        train_i = train_i.loc[:, train[i].columns]\n",
    "        valid_i = valid_i.loc[:, valid[i].columns]\n",
    "        test_i = test_i.loc[:, test[0].columns]\n",
    "\n",
    "        # save\n",
    "        train_scaled.append(train_i)\n",
    "        valid_scaled.append(valid_i)\n",
    "        test_scaled.append(test_i)\n",
    "\n",
    "        scalers.append(scaler)\n",
    "\n",
    "    return train_scaled, valid_scaled, test_scaled, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mbpLP_tZ0KTt"
   },
   "outputs": [],
   "source": [
    "def test_dup(test, train):\n",
    "  columns = train[0].columns.drop([\"datetime\", 'y'])\n",
    "  test_scaled = []\n",
    "  for i in range(len(train)):\n",
    "\n",
    "    # standardize the sets (-> ndarray) without datetime\n",
    "    test_i = test[0].copy().drop([\"datetime\", 'y'], axis=1)\n",
    "\n",
    "    # recreate dataframe\n",
    "    test_i = pd.DataFrame(data=test_i, columns=columns)\n",
    "\n",
    "    # add datetime\n",
    "    test_i[\"datetime\"] = pd.DatetimeIndex(test[0].loc[:, \"datetime\"].values)\n",
    "    test_i[\"y\"] = test[0].loc[:, \"y\"].values\n",
    "\n",
    "    # reorder\n",
    "    test_i = test_i.loc[:, test[0].columns]\n",
    "\n",
    "    test_i = test_i.reset_index(drop=True)\n",
    "\n",
    "    # save\n",
    "    test_scaled.append(test_i)\n",
    "\n",
    "  return test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHA7RYc5Hh6Q"
   },
   "source": [
    "###Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qENptNmm2cTZ",
    "outputId": "b3d30cc6-99d9-422a-de1e-74305417649d"
   },
   "outputs": [],
   "source": [
    "data = load_ohio(dataset, subject)\n",
    "data_resampled = resample(data, freq)\n",
    "data_sampled = create_samples(data_resampled, ph, hist, day_len_freq)\n",
    "data_filled_na = fill_nans(data_sampled, day_len_freq, n_days_test)\n",
    "train, valid, test = split(data_filled_na, day_len_freq, n_days_test, cv)\n",
    "[train, valid, test] = [remove_nans(set) for set in [train, valid, test]]\n",
    "test = test_dup(test, train)\n",
    "\n",
    "# train, valid, test, scalers = standardize(train, valid, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZMeiR40U7yO"
   },
   "outputs": [],
   "source": [
    "Initial_features_train = train[0].columns\n",
    "Initial_features_valid = valid[0].columns\n",
    "Initial_features_test = test[0].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QogXxksgT9LF"
   },
   "source": [
    "##Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HapZNAy7UXM"
   },
   "outputs": [],
   "source": [
    "hovorka_inputs_train = []\n",
    "for i in range(len(train)):\n",
    "  hovorka_inputs_train.append(aggregate_inputs(train[i]))\n",
    "\n",
    "hovorka_inputs_valid = []\n",
    "for i in range(len(valid)):\n",
    "  hovorka_inputs_valid.append(aggregate_inputs(valid[i]))\n",
    "\n",
    "hovorka_inputs_test = []\n",
    "for i in range(len(test)):\n",
    "  hovorka_inputs_test.append(aggregate_inputs(test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XavDdeW6phwU"
   },
   "outputs": [],
   "source": [
    "# Constants from Hovorka model\n",
    "G0 = 5\n",
    "Bow = 70\n",
    "k12 = 0.066\n",
    "ka1 = 0.006\n",
    "ka2 = 0.06\n",
    "ka3 = 0.03\n",
    "kb1 = 3.072 * 10**-5\n",
    "kb2 = 4.92 * 10**-5\n",
    "kb3 = 0.00156\n",
    "ke = 0.138\n",
    "VI = 0.12 * Bow\n",
    "VG = 0.16 * Bow\n",
    "AG = 0.8\n",
    "tmax_G = 40\n",
    "tmax_I = 55\n",
    "SFIT = 51.2 * 10**-4\n",
    "SFID = 8.2 * 10**-4\n",
    "SFIE = 520 * 10**-4\n",
    "EGP0 = 0.0161 * Bow\n",
    "F01 = 0.0097 * Bow\n",
    "Q10 = G0 * VG\n",
    "u0 = 6.68\n",
    "S10 = u0 * tmax_I\n",
    "S20 = S10\n",
    "Ui0 = S20/tmax_I\n",
    "I0 = Ui0/(ke * VI)\n",
    "x10 = SFIT * I0\n",
    "x20 = SFID * I0\n",
    "x30 = SFIE * I0\n",
    "Q20 = x10 / (x20 + k12)\n",
    "\n",
    "preprocessed_data_w_hovorka_train = []\n",
    "preprocessed_data_w_hovorka_valid = []\n",
    "preprocessed_data_w_hovorka_test = []\n",
    "\n",
    "\n",
    "# Function to define Hovorka model equations\n",
    "def hovorka_model(y, t, d, u):\n",
    "    Q1, Q2, x1, x2, x3, D1, D2, S1, S2, I = y\n",
    "\n",
    "    # Glucose absorption compartments\n",
    "    D = 1000 * 180.156 * np.interp(t, np.arange(len(d)), d)\n",
    "    dD1_dt = AG * D - D1 / tmax_G\n",
    "    dD2_dt = D1 / tmax_G - D2 / tmax_G\n",
    "    UG = D2 / tmax_G\n",
    "\n",
    "    # Amount of glucose\n",
    "    F01_c = F01 if Q1 / VG >= 4.5 else F01 * (Q1 / VG / 4.5)\n",
    "    FR = 0.003 * (Q1 / VG - 9) * VG if Q1 / VG >= 9 else 0\n",
    "\n",
    "    # Amount of glucose\n",
    "    dQ1_dt = UG - x1 * Q1 - F01_c - FR + k12 * Q2 + EGP0 * (1 - x3)\n",
    "    dQ2_dt = x1 * Q1 - (k12 + x2) * Q2\n",
    "    G = Q1 / VG\n",
    "\n",
    "    # Insulin effects\n",
    "    dx1_dt = -ka1 * x1 + kb1 * I\n",
    "    dx2_dt = -ka2 * x2 + kb2 * I\n",
    "    dx3_dt = -ka3 * x3 + kb3 * I\n",
    "\n",
    "    # Insulin absorption compartments\n",
    "    dS1_dt = np.interp(t, np.arange(len(u)), u) - S1 / tmax_I\n",
    "    dS2_dt = S1 / tmax_I - S2 / tmax_I\n",
    "    dI_dt = S2 / tmax_I / VI - ke * I\n",
    "\n",
    "    return [dQ1_dt, dQ2_dt, dx1_dt, dx2_dt, dx3_dt, dD1_dt, dD2_dt, dS1_dt, dS2_dt, dI_dt]\n",
    "# Extract the relevant columns\n",
    "# flag =0\n",
    "for (k,j) in ((hovorka_inputs_train, train), (hovorka_inputs_valid, valid), (hovorka_inputs_test, test)):\n",
    "  for i in range(len(k)):\n",
    "    # print(i)\n",
    "    # if k is hovorka_inputs_valid and i == 2:\n",
    "    #   flag = 1\n",
    "    glucose_avg = k[i]['glucose_input'].values\n",
    "    CHO_sum = k[i]['CHO_input'].values\n",
    "    insulin_sum = k[i]['insulin_input'].values\n",
    "\n",
    "    # Define the time points (assuming they are evenly spaced)\n",
    "    time_points = np.arange(len(k[i]))\n",
    "    initial_conditions = [Q10, Q20, x10, x20, x30, 0, 0, S10, S20, I0]\n",
    "\n",
    "    # pdb.set_trace()\n",
    "    # Integrate the Hovorka model equations over time\n",
    "    results = odeint(hovorka_model, initial_conditions, time_points, args=(CHO_sum, insulin_sum))\n",
    "\n",
    "    # Extracting features\n",
    "    G_t = results[:, 0] / VG\n",
    "    I_t = results[:, -1]\n",
    "    UG_t = results[:, 6] / tmax_G\n",
    "\n",
    "    # Create a dataframe with the results\n",
    "    hovorka_features = pd.DataFrame({\n",
    "        'G_t': G_t,\n",
    "        'I_t': I_t,\n",
    "        'UG_t': UG_t\n",
    "    }, index=j[i].index)\n",
    "\n",
    "    # print(hovorka_features)\n",
    "    # print(train[i])\n",
    "    if j is train:\n",
    "      # Combine with preprocessed data if needed\n",
    "      preprocessed_data_w_hovorka_train.append(pd.concat([j[i], hovorka_features], axis=1))\n",
    "      # print(preprocessed_data_with_hovorka)\n",
    "    elif j is valid:\n",
    "      preprocessed_data_w_hovorka_valid.append(pd.concat([j[i], hovorka_features], axis=1))\n",
    "    else:\n",
    "      preprocessed_data_w_hovorka_test.append(pd.concat([j[i], hovorka_features], axis=1))\n",
    "\n",
    "# print(preprocessed_data_w_hovorka_train[0])\n",
    "# print(preprocessed_data_w_hovorka_valid[0])\n",
    "# print(preprocessed_data_w_hovorka_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXSb19PxlJIi"
   },
   "outputs": [],
   "source": [
    "for (k,l) in ((preprocessed_data_w_hovorka_train, train), (preprocessed_data_w_hovorka_valid, valid), (preprocessed_data_w_hovorka_test, test)):\n",
    "\n",
    "  l_g = [pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()]\n",
    "  l_c = [pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()]\n",
    "  l_i = [pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()]\n",
    "  l_y = [pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()]\n",
    "  for j in range(len(k)):\n",
    "\n",
    "    n_samples = k[j].shape[0] - ph - hist + 1\n",
    "\n",
    "    g = np.array([k[j].loc[i:i + n_samples - 1, \"G_t\"] for i in range(hist)]).transpose()\n",
    "    i_n = np.array([k[j].loc[i:i + n_samples - 1, \"I_t\"] for i in range(hist)]).transpose()\n",
    "    c = np.array([k[j].loc[i:i + n_samples - 1, \"UG_t\"] for i in range(hist)]).transpose()\n",
    "\n",
    "    new_columns = np.r_[[\"G_t\" + str(i) for i in range(hist)], [\"I_t\" + str(i) for i in range(hist)], [\n",
    "          \"UG_t\" + str(i) for i in range(hist)]]\n",
    "    new_data = pd.DataFrame(data=np.c_[g, i_n, c], columns=new_columns)\n",
    "    # Assuming 'train' is your list of DataFrames with original sequential data\n",
    "    # Merge Hovorka sequences with each DataFrame in train\n",
    "    # pdb.set_trace()\n",
    "    l_g[j] = l[j].loc[:, :'glucose_35']\n",
    "    l_c[j] = l[j].loc[:, 'CHO_0':'CHO_35']\n",
    "    l_i[j] = l[j].loc[:, 'insulin_0':'insulin_35']\n",
    "    l_y[j] = l[j].loc[:, 'y']\n",
    "    new_data_g = new_data.loc[:, 'G_t0':'G_t35']\n",
    "    new_data_c = new_data.loc[:, 'UG_t0':'UG_t35']\n",
    "    new_data_i = new_data.loc[:, 'I_t0':'I_t35']\n",
    "    l[j] = pd.concat([l_g[j], new_data_g.iloc[:len(l[j])].reset_index(drop=True), l_c[j], new_data_c.iloc[:len(l[j])].reset_index(drop=True), l_i[j], new_data_i.iloc[:len(l[j])].reset_index(drop=True), l_y[j]], axis=1)\n",
    "    list_of_cols = l[j].loc[:0, 'G_t0':'UG_t35'].columns  # retrieve only the 0th row for efficiency\n",
    "    l[j].dropna(subset = list_of_cols, inplace = True)\n",
    "# Now each DataFrame in 'train' includes the original sequences and the new Hovorka model feature sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9zhYqq5M9U05"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# # Combine all your training data into a single DataFrame\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "train, valid, test, scalers = standardize(train, valid, test)\n",
    "\n",
    "train_data = train[0]\n",
    "valid_data = valid[0]\n",
    "train_data = pd.concat([train_data, valid_data], ignore_index=True)\n",
    "test_data = test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dJWoDHws2phM"
   },
   "outputs": [],
   "source": [
    "X_train = train_data.drop(['y', 'datetime', 'time'], axis=1).values\n",
    "y_train = train_data['y'].values\n",
    "\n",
    "X_train = X_train.reshape(-1, 36, 6)\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "\n",
    "X_test = test_data.drop(['y', 'datetime', 'time'], axis=1).values\n",
    "y_test = test_data['y'].values\n",
    "\n",
    "X_test = X_test.reshape(-1, 36, 6)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbUNSDG3vHRJ"
   },
   "source": [
    "###after tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_sim.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e5as1oaCO5G",
    "outputId": "cb22c4ef-db48-415a-ff06-e945bf2098dc"
   },
   "outputs": [],
   "source": [
    "history_real = model.fit(X_train, y_train, epochs=20, batch_size=32, callbacks=[early_stopping_real, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_finetuned.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tOtZOsrDB6t"
   },
   "outputs": [],
   "source": [
    "model.load_weights('model_finetuned.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "id": "orAIeTF2p9h4",
    "outputId": "eac3fd87-4354-4994-f8a1-436dac9a7862"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluate\n",
    "test_loss, test_rmse, test_mape = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {test_loss}', f'Test MAE: {test_mape}', f'Test RMSE: {test_rmse}')\n",
    "\n",
    "# Plotting the results\n",
    "plt.figure(figsize=(30, 6))\n",
    "plt.plot(y_test, color='blue', label='Actual BG')\n",
    "plt.plot(y_pred, color='red', label='Predicted BG')\n",
    "plt.title('BG Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('BG')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'datetime' column to pandas datetime format if it's not already\n",
    "test_data['datetime'] = pd.to_datetime(test_data['datetime'])\n",
    "# train_data\n",
    " \n",
    "\n",
    "# Filter the data for the date '20-10-2021'\n",
    "date_filter = '2022-01-06'\n",
    "filtered_data = test_data[test_data['datetime'].dt.strftime('%Y-%m-%d') == date_filter]\n",
    "# filtered_data\n",
    "\n",
    "# # Extract the filtered actual and predicted BG levels\n",
    "y_test_filtered = y_test[filtered_data.index]\n",
    "y_pred_filtered = y_pred[filtered_data.index]\n",
    "\n",
    "# # Plotting the results for the filtered date\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(filtered_data['datetime'], y_test_filtered, color='blue', label='Actual BG')\n",
    "plt.plot(filtered_data['datetime'], y_pred_filtered, color='red', label='Predicted BG')\n",
    "plt.title('BG Prediction for 06-01-2022')\n",
    "plt.xlabel('Date & Time')\n",
    "plt.ylabel('BG (mg/dL)')\n",
    "plt.legend()\n",
    "plt.savefig('Exp3_part1_584.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQtXn7iHPLlg"
   },
   "source": [
    "###CG EGA code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6CmzetQgNvzH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def derivatives(y, freq):\n",
    "    \"\"\"\n",
    "    Compute the derivative of the signal y, (y[i] - y[i-1]) / (t[i] - t[i-1]).\n",
    "    :param y: signal, either true or predicted glucose values;\n",
    "    :return: the derivatives and its associated signal (every sample has a corresponding derivative)\n",
    "    \"\"\"\n",
    "    dy = np.reshape(np.diff(y, axis=1), (-1, 1)) / freq\n",
    "    y = np.reshape(y[:, 1:], (-1, 1))\n",
    "    return dy, y\n",
    "\"\"\"\n",
    "    This file contains AP/BE/EP filters for every glycemia regions. It is used by the CG-EGA object.\n",
    "\"\"\"\n",
    "\n",
    "filter_AP_hypo = [\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "]\n",
    "\n",
    "filter_BE_hypo = [\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [1, 0, 0],\n",
    "]\n",
    "\n",
    "filter_EP_hypo = [\n",
    "    [0, 1, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 1, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [0, 1, 1],\n",
    "    [1, 1, 1],\n",
    "    [0, 1, 1],\n",
    "]\n",
    "\n",
    "filter_AP_eu = [\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "]\n",
    "\n",
    "filter_BE_eu = [\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 0],\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 0],\n",
    "]\n",
    "\n",
    "filter_EP_eu = [\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [0, 0, 1],\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 1],\n",
    "]\n",
    "\n",
    "filter_AP_hyper = [\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "]\n",
    "\n",
    "filter_BE_hyper = [\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "]\n",
    "\n",
    "filter_EP_hyper = [\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [0, 0, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tg8RbC4jNhAd"
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def _any(l, axis=1):\n",
    "    return np.reshape(np.any(np.concatenate(l, axis=axis), axis=axis), (-1, 1))\n",
    "\n",
    "\n",
    "def _all(l, axis=1):\n",
    "    return np.reshape(np.all(np.concatenate(l, axis=axis), axis=axis), (-1, 1))\n",
    "\n",
    "\n",
    "def reshape_results(results, freq):\n",
    "    # resampling\n",
    "    if type(results.index) != type(date.today()): # in case the results objects isn't formatted with dates\n",
    "        start_time = date.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        end_time = (datetime.now().replace(hour=0,minute=0,second=0,microsecond=0) + timedelta(minutes=freq*(len(results.index)-1))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        index = pd.period_range(start=start_time, end=end_time, freq=str(freq) + 'min').to_timestamp()\n",
    "        results.index = index\n",
    "    else:\n",
    "        start_time = datetime.strptime(results.index[0].strftime('%Y-%m-%d'), '%Y-%m-%d').strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        end_time = (datetime.strptime(results.index[-1].strftime('%Y-%m-%d'), '%Y-%m-%d') + timedelta(days=1) - timedelta(\n",
    "            minutes=float(freq))).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        index = pd.period_range(start=start_time, end=end_time, freq=str(freq) + 'min').to_timestamp()\n",
    "        results = results.resample(str(freq) + 'min').mean()\n",
    "        results = results.reindex(index=index)\n",
    "\n",
    "    # creating the derivatives\n",
    "    results = pd.concat([results, (results.diff(axis=0) / freq).rename(columns={\"y_true\": \"dy_true\", \"y_pred\": \"dy_pred\"})],\n",
    "                        axis=1)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def extract_columns_from_results(results):\n",
    "    return np.expand_dims(results.dropna().to_numpy().transpose(),axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHv6xoGlGMTr"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class P_EGA():\n",
    "    \"\"\"\n",
    "        The Point-Error Grid Analysis (P-EGA) gives an estimation of the clinical acceptability of the glucose\n",
    "        predictions based on their point-accuracy. It is also known as the Clarke Error Grid Analysis (Clarke EGA).\n",
    "        Every prediction is given a mark from A to E depending of the ground truth.\n",
    "\n",
    "        This implementation follows \"Evaluating the accuracy of continuous glucose-monitoring sensors: continuous\n",
    "        glucose-error grid analysis illustrated by TheraSense Freestyle Navigator data.\", Kovatchev et al., 2004.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, results, freq):\n",
    "        \"\"\"\n",
    "        Instantiate the P-EGA object.\n",
    "        :param results: dataframe with predictions and ground truths\n",
    "        :param freq: prediction frequency in minutes (e.g., 5)\n",
    "        \"\"\"\n",
    "\n",
    "        self.freq = freq\n",
    "        self.results = reshape_results(results, self.freq)\n",
    "\n",
    "    def full(self):\n",
    "        \"\"\"\n",
    "            Full version of the P-EGA, which consists of an array giving for every prediction (row), its mark vector\n",
    "            (column). There are 5 columns representing the mark A, B, C, D, and E.\n",
    "\n",
    "            :return: numy array of shape (number of predictions, 5)\n",
    "        \"\"\"\n",
    "        y_true, y_pred, dy_true, dy_pred = extract_columns_from_results(self.results)\n",
    "\n",
    "        # if the true rate are big, we accept bigger mistake (region borders are modified)\n",
    "        mod = np.zeros_like(y_true)\n",
    "        mod[_any([\n",
    "            _all([\n",
    "                np.greater(dy_true, -2),\n",
    "                np.less_equal(dy_true, -1)]),\n",
    "            _all([\n",
    "                np.less(dy_true, 2),\n",
    "                np.greater_equal(dy_true, 1),\n",
    "            ])\n",
    "        ])] = 10\n",
    "\n",
    "        mod[_any([\n",
    "            _all([\n",
    "                np.less_equal(dy_true, -2)\n",
    "            ]),\n",
    "            _all([\n",
    "                np.greater_equal(dy_true, 2)\n",
    "            ])\n",
    "        ])] = 20\n",
    "\n",
    "        A = _any([\n",
    "            _all([\n",
    "                np.less_equal(y_pred, 70 + mod),\n",
    "                np.less_equal(y_true, 70)\n",
    "            ]),\n",
    "            _all([\n",
    "                np.less_equal(y_pred, y_true * 6 / 5 + mod),\n",
    "                np.greater_equal(y_pred, y_true * 4 / 5 - mod)\n",
    "            ])\n",
    "        ])\n",
    "\n",
    "        E = _any([\n",
    "            _all([\n",
    "                np.greater(y_true, 180),\n",
    "                np.less(y_pred, 70 - mod)\n",
    "            ]),\n",
    "            _all([\n",
    "                np.greater(y_pred, 180 + mod),\n",
    "                np.less_equal(y_true, 70)\n",
    "            ])\n",
    "        ])\n",
    "\n",
    "        D = _any([\n",
    "            _all([\n",
    "                np.greater(y_pred, 70 + mod),\n",
    "                np.greater(y_pred, y_true * 6 / 5 + mod),\n",
    "                np.less_equal(y_true, 70),\n",
    "                np.less_equal(y_pred, 180 + mod)\n",
    "            ]),\n",
    "            _all([\n",
    "                np.greater(y_true, 240),\n",
    "                np.less(y_pred, 180 - mod),\n",
    "                np.greater_equal(y_pred, 70 - mod)\n",
    "            ])\n",
    "        ])\n",
    "\n",
    "        C = _any([\n",
    "            _all([\n",
    "                np.greater(y_true, 70),\n",
    "                np.greater(y_pred, y_true * 22 / 17 + (180 - 70 * 22 / 17) + mod)\n",
    "            ]),\n",
    "            _all([\n",
    "                np.less_equal(y_true, 180),\n",
    "                np.less(y_pred, y_true * 7 / 5 - 182 - mod)\n",
    "            ])\n",
    "        ])\n",
    "\n",
    "        # B being the weirdest zone in the P-EGA, we compute it last by saying\n",
    "        # it's all the points that have not been classified yet.\n",
    "        B = _all([\n",
    "            np.equal(A, False),\n",
    "            np.equal(C, False),\n",
    "            np.equal(D, False),\n",
    "            np.equal(E, False),\n",
    "        ])\n",
    "\n",
    "        return np.concatenate([A, B, C, D, E], axis=1)\n",
    "\n",
    "    def mean(self):\n",
    "        return np.mean(self.full(), axis=0)\n",
    "\n",
    "    def a_plus_b(self):\n",
    "        full = self.full()\n",
    "        a_plus_b = full[:, 0] + full[:, 1]\n",
    "        return np.sum(a_plus_b) / len(a_plus_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tC-nHhW-N7mv"
   },
   "outputs": [],
   "source": [
    "class R_EGA():\n",
    "    \"\"\"\n",
    "        The Rate-Error Grid Analysis (P-EGA) gives an estimation of the clinical acceptability of the glucose\n",
    "        predictions based on their rate-of-change-accuracy (the accuracy of the predicted variations).\n",
    "        Every prediction is given a mark from {\"A\", \"B\", \"uC\", \"lC\", \"uD\", \"lD\", \"uE\", \"lE\"}\n",
    "\n",
    "        The implementation is taken from \"Evaluating the accuracy of continuous glucose-monitoring sensors: continuous\n",
    "        glucose-error grid analysis illustrated by TheraSense Freestyle Navigator data.\", Kovatchev et al., 2004.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, results, freq):\n",
    "        \"\"\"\n",
    "        Instantiate the P-EGA object.\n",
    "        :param results: dataframe with predictions and ground truths\n",
    "        :param freq: prediction frequency in minutes (e.g., 5)\n",
    "        \"\"\"\n",
    "\n",
    "        self.freq = freq\n",
    "        self.results = reshape_results(results, self.freq)\n",
    "\n",
    "    def full(self):\n",
    "        \"\"\"\n",
    "            Full version of the R-EGA, which consists of an array giving for every prediction (row), its mark vector\n",
    "            (column). There are 8 columns representing the mark A, B, uC, lC, uD, lD, uE, and lE.\n",
    "\n",
    "            :return: numy array of shape (number of predictions, 8)\n",
    "        \"\"\"\n",
    "        y_true, y_pred, dy_true, dy_pred = extract_columns_from_results(self.results)\n",
    "\n",
    "        A = _any([\n",
    "            _all([  # upper and lower\n",
    "                np.greater_equal(dy_pred, dy_true - 1),\n",
    "                np.less_equal(dy_pred, dy_true + 1)\n",
    "            ]),\n",
    "            _all([  # left\n",
    "                np.less_equal(dy_pred, dy_true / 2),\n",
    "                np.greater_equal(dy_pred, dy_true * 2)\n",
    "            ]),\n",
    "            _all([  # right\n",
    "                np.less_equal(dy_pred, dy_true * 2),\n",
    "                np.greater_equal(dy_pred, dy_true / 2)\n",
    "            ])\n",
    "        ])\n",
    "\n",
    "        B = _all([\n",
    "            np.equal(A, False),  # not in A but satisfies the cond below\n",
    "            _any([\n",
    "                _all([\n",
    "                    np.less_equal(dy_pred, -1),\n",
    "                    np.less_equal(dy_true, -1)\n",
    "                ]),\n",
    "                _all([\n",
    "                    np.less_equal(dy_pred, dy_true + 2),\n",
    "                    np.greater_equal(dy_pred, dy_true - 2)\n",
    "                ]),\n",
    "                _all([\n",
    "                    np.greater_equal(dy_pred, 1),\n",
    "                    np.greater_equal(dy_true, 1)\n",
    "                ])\n",
    "            ])\n",
    "        ])\n",
    "\n",
    "        uC = _all([\n",
    "            np.less(dy_true, 1),\n",
    "            np.greater_equal(dy_true, -1),\n",
    "            np.greater(dy_pred, dy_true + 2)\n",
    "        ])\n",
    "\n",
    "        lC = _all([\n",
    "            np.less_equal(dy_true, 1),\n",
    "            np.greater(dy_true, -1),\n",
    "            np.less(dy_pred, dy_true - 2)\n",
    "        ])\n",
    "\n",
    "        uD = _all([\n",
    "            np.less_equal(dy_pred, 1),\n",
    "            np.greater_equal(dy_pred, -1),\n",
    "            np.greater(dy_pred, dy_true + 2)\n",
    "        ])\n",
    "\n",
    "        lD = _all([\n",
    "            np.less_equal(dy_pred, 1),\n",
    "            np.greater_equal(dy_pred, -1),\n",
    "            np.less(dy_pred, dy_true - 2)\n",
    "        ])\n",
    "\n",
    "        uE = _all([\n",
    "            np.greater(dy_pred, 1),\n",
    "            np.less(dy_true, -1)\n",
    "        ])\n",
    "\n",
    "        lE = _all([\n",
    "            np.less(dy_pred, -1),\n",
    "            np.greater(dy_true, 1)\n",
    "        ])\n",
    "\n",
    "        return np.concatenate([A, B, uC, lC, uD, lD, uE, lE], axis=1)\n",
    "\n",
    "    def mean(self):\n",
    "        return np.mean(self.full(), axis=0)\n",
    "\n",
    "    def a_plus_b(self):\n",
    "        full = self.full()\n",
    "        a_plus_b = full[:, 0] + full[:, 1]\n",
    "        return np.sum(a_plus_b) / len(a_plus_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sGWGOviqM0D4"
   },
   "outputs": [],
   "source": [
    "class CG_EGA():\n",
    "    \"\"\"\n",
    "        The Continuous Glucose-Error Grid Analysis (CG-EGA) gives a measure of the clinical acceptability of the glucose predictions. It analyzes both the\n",
    "        prediction accuracy (through the P-EGA) and the predicted variation accuracy (R-EGA).\n",
    "\n",
    "        The implementation has been made following \"Evaluating the accuracy of continuous glucose-monitoring sensors:\n",
    "        continuous glucose-error grid analysis illustrated by TheraSense Freestyle Navigator data.\", Kovatchev et al., 2004.\n",
    "    \"\"\"\n",
    "\n",
    "    day_len = 1440\n",
    "\n",
    "    def __init__(self, results, freq):\n",
    "        \"\"\"\n",
    "        Instantiate the CG-EGA object.\n",
    "        :param results: dataframe with predictions and ground truths\n",
    "        :param freq: prediction frequency in minutes (e.g., 5)\n",
    "        \"\"\"\n",
    "        self.results = reshape_results(results, freq)\n",
    "        self.freq = freq\n",
    "        self.day_len = self.day_len // freq\n",
    "        self.p_ega = P_EGA(results, freq).full()\n",
    "        self.r_ega = R_EGA(results, freq).full()\n",
    "\n",
    "\n",
    "    def full(self):\n",
    "        \"\"\"\n",
    "            Full version of the CG-EGA, which consists of 3 tables (representing the hypoglycemia, euglycemia, and\n",
    "            hyperglycemia regions) being the cartesian product between the P-EGA and the R-EGA. Every cell contains the\n",
    "            number of predictions falling into it.\n",
    "\n",
    "            :return: hypoglycemia full CG-EGA, euglycemia full CG-EGA, hyperglycemia full CG-EGA\n",
    "        \"\"\"\n",
    "        y_true, y_pred, dy_true, dy_pred = extract_columns_from_results(self.results)\n",
    "        p_ega, r_ega = self.p_ega, self.r_ega\n",
    "\n",
    "        # compute the glycemia regions\n",
    "        hypoglycemia = np.less_equal(y_true, 70).reshape(-1, 1)\n",
    "        euglycemia = _all([\n",
    "            np.less_equal(y_true, 180),\n",
    "            np.greater(y_true, 70)\n",
    "        ]).reshape(-1, 1)\n",
    "        hyperglycemia = np.greater(y_true, 180).reshape(-1, 1)\n",
    "\n",
    "        # apply region filter and convert to 0s and 1s\n",
    "        P_hypo = np.reshape(np.concatenate([np.reshape(p_ega[:, 0], (-1, 1)),\n",
    "                                            np.reshape(p_ega[:, 3], (-1, 1)),\n",
    "                                            np.reshape(p_ega[:, 4], (-1, 1))], axis=1).astype(\"int32\") *\n",
    "                            hypoglycemia.astype(\"int32\"),\n",
    "                            (-1, 3))\n",
    "        P_eu = np.reshape(np.concatenate([np.reshape(p_ega[:, 0], (-1, 1)),\n",
    "                                          np.reshape(p_ega[:, 1], (-1, 1)),\n",
    "                                          np.reshape(p_ega[:, 2], (-1, 1))], axis=1).astype(\"int32\") *\n",
    "                          euglycemia.astype(\"int32\"),\n",
    "                          (-1, 3))\n",
    "        P_hyper = np.reshape(p_ega.astype(\"int32\") * hyperglycemia.astype(\"int32\"), (-1, 5))\n",
    "\n",
    "        R_hypo = np.reshape(r_ega.astype(\"int32\") * hypoglycemia.astype(\"int32\"), (-1, 8))\n",
    "        R_eu = np.reshape(r_ega.astype(\"int32\") * euglycemia.astype(\"int32\"), (-1, 8))\n",
    "        R_hyper = np.reshape(r_ega.astype(\"int32\") * hyperglycemia.astype(\"int32\"), (-1, 8))\n",
    "\n",
    "        CG_EGA_hypo = np.dot(np.transpose(R_hypo), P_hypo)\n",
    "        CG_EGA_eu = np.dot(np.transpose(R_eu), P_eu)\n",
    "        CG_EGA_hyper = np.dot(np.transpose(R_hyper), P_hyper)\n",
    "\n",
    "        return CG_EGA_hypo, CG_EGA_eu, CG_EGA_hyper\n",
    "\n",
    "    def simplified(self, count=False):\n",
    "        \"\"\"\n",
    "            Simplifies the full CG-EGA into Accurate Prediction (AP), Benign Prediction (BE), and Erroneous Prediction (EP)\n",
    "            rates for every glycemia regions.\n",
    "\n",
    "            :param count: if False, the results, for every region, will be expressed as a ratio\n",
    "\n",
    "            :return: AP rate in hypoglycemia, BE rate in hypoglycemia, EP rate in hypoglycemia,\n",
    "                     AP rate in euglycemia, BE rate in euglycemia, EP rate in euglycemia,\n",
    "                     AP rate in hyperglycemia, BE rate in hyperglycemia, EP rate in hyperglycemia\n",
    "        \"\"\"\n",
    "\n",
    "        CG_EGA_hypo, CG_EGA_eu, CG_EGA_hyper = self.full()\n",
    "\n",
    "        AP_hypo = np.sum(CG_EGA_hypo * filter_AP_hypo)\n",
    "        BE_hypo = np.sum(CG_EGA_hypo * filter_BE_hypo)\n",
    "        EP_hypo = np.sum(CG_EGA_hypo * filter_EP_hypo)\n",
    "\n",
    "        AP_eu = np.sum(CG_EGA_eu * filter_AP_eu)\n",
    "        BE_eu = np.sum(CG_EGA_eu * filter_BE_eu)\n",
    "        EP_eu = np.sum(CG_EGA_eu * filter_EP_eu)\n",
    "\n",
    "        AP_hyper = np.sum(CG_EGA_hyper * filter_AP_hyper)\n",
    "        BE_hyper = np.sum(CG_EGA_hyper * filter_BE_hyper)\n",
    "        EP_hyper = np.sum(CG_EGA_hyper * filter_EP_hyper)\n",
    "\n",
    "        if not count:\n",
    "            sum_hypo = (AP_hypo + BE_hypo + EP_hypo)\n",
    "            sum_eu = (AP_eu + BE_eu + EP_eu)\n",
    "            sum_hyper = (AP_hyper + BE_hyper + EP_hyper)\n",
    "\n",
    "\n",
    "            [AP_hypo, BE_hypo, EP_hypo] = [AP_hypo / sum_hypo, BE_hypo / sum_hypo, EP_hypo / sum_hypo] if not sum_hypo == 0 else [np.nan, np.nan, np.nan]\n",
    "            [AP_eu, BE_eu, EP_eu] = [AP_eu / sum_eu, BE_eu / sum_eu, EP_eu / sum_eu] if not sum_eu == 0 else [np.nan, np.nan, np.nan]\n",
    "            [AP_hyper, BE_hyper, EP_hyper] = [AP_hyper / sum_hyper, BE_hyper / sum_hyper, EP_hyper / sum_hyper] if not sum_hyper == 0 else [np.nan, np.nan, np.nan]\n",
    "\n",
    "        return AP_hypo, BE_hypo, EP_hypo, AP_eu, BE_eu, EP_eu, AP_hyper, BE_hyper, EP_hyper\n",
    "\n",
    "    def reduced(self):\n",
    "        \"\"\"\n",
    "            Reduces the simplified CG-EGA by not dividing the results into the glycemia regions\n",
    "            :return: overall AP rate, overall BE rate, overall EP rate\n",
    "        \"\"\"\n",
    "\n",
    "        AP_hypo, BE_hypo, EP_hypo, AP_eu, BE_eu, EP_eu, AP_hyper, BE_hyper, EP_hyper = self.simplified(count=True)\n",
    "        sum = (AP_hypo + BE_hypo + EP_hypo + AP_eu + BE_eu + EP_eu + AP_hyper + BE_hyper + EP_hyper)\n",
    "        return (AP_hypo + AP_eu + AP_hyper) / sum, (BE_hypo + BE_eu + BE_hyper) / sum, (\n",
    "                EP_hypo + EP_eu + EP_hyper) / sum\n",
    "\n",
    "    def per_sample(self):\n",
    "        \"\"\"\n",
    "            Compute the per-sample simplified CG-EGA\n",
    "            :return: pandas DataFrame with columns (y_true, y_pred, dy_true, dy_pred, P-EGA mark, R-EGA mark,\n",
    "                                                    CG-EGA AP/BE/EP mark)\n",
    "        \"\"\"\n",
    "\n",
    "        y_true, y_pred, dy_true, dy_pred = extract_columns_from_results(self.results)\n",
    "        p_ega, r_ega = self.p_ega, self.r_ega\n",
    "\n",
    "        df = pd.DataFrame(data=np.c_[y_true, dy_true, y_pred, dy_pred, p_ega, r_ega])\n",
    "        df[\"CG_EGA\"] = len(df.index) * [\"?\"]\n",
    "\n",
    "        df.columns = [\"y_true\", \"dy_true\", \"y_pred\", \"dy_pred\", \"P_A\", \"P_B\", \"P_C\", \"P_D\", \"P_E\", \"R_A\", \"R_B\", \"R_uC\",\n",
    "                      \"R_lC\", \"R_uD\", \"R_lD\", \"R_uE\", \"R_lE\", \"CG_EGA\"]\n",
    "\n",
    "        p_ega_labels = [\"A\", \"B\", \"C\", \"D\", \"E\"]\n",
    "        r_ega_labels = [\"A\", \"B\", \"uC\", \"lC\", \"uD\", \"lD\", \"uE\", \"lE\"]\n",
    "\n",
    "        df[\"time\"] = (np.arange(len(df.index)) + 1) * self.freq\n",
    "\n",
    "        for i in range(len(p_ega)):\n",
    "            p_ega_i = df.iloc[i, 4:9].values.reshape(1, -1)\n",
    "            r_ega_i = df.iloc[i, 9:17].values.reshape(1, -1)\n",
    "            y_true_i = df.iloc[i, 0]\n",
    "\n",
    "            cg_ega_i = np.dot(np.transpose(r_ega_i), p_ega_i)\n",
    "\n",
    "            if y_true_i <= 70:\n",
    "                # hypoglycemia\n",
    "                cg_ega_i = cg_ega_i[:, [0, 3, 4]]\n",
    "\n",
    "                if np.sum(cg_ega_i * filter_AP_hypo) == 1:\n",
    "                    df.loc[i, \"CG_EGA\"] = \"AP\"\n",
    "                elif np.sum(cg_ega_i * filter_BE_hypo) == 1:\n",
    "                    df.loc[i, \"CG_EGA\"] = \"BE\"\n",
    "                else:\n",
    "                    df.loc[i, \"CG_EGA\"] = \"EP\"\n",
    "\n",
    "            elif y_true_i <= 180:\n",
    "                # euglycemia\n",
    "                cg_ega_i = cg_ega_i[:, [0, 1, 2]]\n",
    "\n",
    "                if np.sum(cg_ega_i * filter_AP_eu) == 1:\n",
    "                    df.loc[i, \"CG_EGA\"] = \"AP\"\n",
    "                elif np.sum(cg_ega_i * filter_BE_eu) == 1:\n",
    "                    df.loc[i, \"CG_EGA\"] = \"BE\"\n",
    "                else:\n",
    "                    df.loc[i, \"CG_EGA\"] = \"EP\"\n",
    "            else:\n",
    "                # hyperglycemia\n",
    "                if np.sum(cg_ega_i * filter_AP_hyper) == 1:\n",
    "                    df.loc[i, \"CG_EGA\"] = \"AP\"\n",
    "                elif np.sum(cg_ega_i * filter_BE_hyper) == 1:\n",
    "                    df.loc[i, \"CG_EGA\"] = \"BE\"\n",
    "                else:\n",
    "                    df.loc[i, \"CG_EGA\"] = \"EP\"\n",
    "\n",
    "            df.loc[i, \"P_EGA\"] = p_ega_labels[np.argmax(p_ega_i.ravel())]\n",
    "            df.loc[i, \"R_EGA\"] = r_ega_labels[np.argmax(r_ega_i.ravel())]\n",
    "\n",
    "        df.index = pd.notnull(self.results).all(1).to_numpy().nonzero()[0]\n",
    "        df_nan = pd.concat([\n",
    "            self.results.copy().reset_index().rename(columns={\"index\": \"datetime\"}),\n",
    "            df.loc[:, [\"CG_EGA\", \"P_EGA\", \"R_EGA\"]]\n",
    "        ], axis=1)\n",
    "\n",
    "        return df_nan\n",
    "\n",
    "    def plot(self, day=0):\n",
    "        \"\"\"\n",
    "        Plot the given day predictions and CG-EGA\n",
    "        :param day: (int) number of the day for which to plot the predictions and CG-EGA\n",
    "        :return: /\n",
    "        \"\"\"\n",
    "        res = self.per_sample().iloc[day * self.day_len:(day + 1) * self.day_len - 1]\n",
    "        pd.plotting.register_matplotlib_converters()\n",
    "        ap = res[res[\"CG_EGA\"] == \"AP\"]\n",
    "        be = res[res[\"CG_EGA\"] == \"BE\"]\n",
    "        ep = res[res[\"CG_EGA\"] == \"EP\"]\n",
    "\n",
    "        f = plt.figure(figsize=(10, 4.5))\n",
    "        # ax1 = f.add_subplot(211)\n",
    "        ax2 = f.add_subplot(121)\n",
    "        ax3 = f.add_subplot(122)\n",
    "\n",
    "        # prediction VS truth against time\n",
    "        # ax1.plot(res[\"datetime\"], res[\"y_true\"], \"k\", label=\"y_true\")\n",
    "        # ax1.plot(res[\"datetime\"], res[\"y_pred\"], \"--k\", label=\"y_pred\")\n",
    "        # ax1.plot(ap[\"datetime\"], ap[\"y_pred\"], label=\"AP\", marker=\"o\", mec=\"xkcd:green\", mfc=\"xkcd:green\", ls=\"\")\n",
    "        # ax1.plot(be[\"datetime\"], be[\"y_pred\"], label=\"BE\", marker=\"o\", mec=\"xkcd:orange\", mfc=\"xkcd:orange\", ls=\"\")\n",
    "        # ax1.plot(ep[\"datetime\"], ep[\"y_pred\"], label=\"EP\", marker=\"o\", mec=\"xkcd:red\", mfc=\"xkcd:red\", ls=\"\")\n",
    "        # ax1.set_title(\"Prediction VS ground truth as a function of time\")\n",
    "        # ax1.set_xlabel(\"Time (min)\")\n",
    "        # ax1.set_ylabel(\"Glucose value (mg/dL)\")\n",
    "        # ax1.legend()\n",
    "\n",
    "        # P-EGA structure\n",
    "        ax2.plot([0, 400], [0, 400], \"-k\", linewidth=0.75)\n",
    "        ax2.plot([58.33, 400], [58.33333 * 6 / 5, 400 * 6 / 5], \"-k\")\n",
    "        ax2.plot([0, 58.33333], [70, 70], \"-k\")\n",
    "        ax2.plot([70, 400], [56, 320], \"-k\")\n",
    "        ax2.plot([70, 70], [0, 56], \"-k\")\n",
    "        ax2.plot([70, 70], [84, 400], \"-k\")\n",
    "        ax2.plot([0, 70], [180, 180], \"-k\")\n",
    "        ax2.plot([70, 400], [70 * 22 / 17 + 89.412, 400 * 22 / 17 + 89.412], \"-k\")\n",
    "        ax2.plot([180, 180], [0, 70], \"-k\")\n",
    "        ax2.plot([180, 400], [70, 70], \"-k\")\n",
    "        ax2.plot([240, 240], [70, 180], \"-k\")\n",
    "        ax2.plot([240, 400], [180, 180], \"-k\")\n",
    "        ax2.plot([130, 180], [130 * 7 / 5 - 182, 180 * 7 / 5 - 182], \"-k\")\n",
    "        ax2.plot([130, 180], [130 * 7 / 5 - 202, 180 * 7 / 5 - 202], \"--k\")\n",
    "        ax2.plot([180, 400], [50, 50], \"--k\")\n",
    "        ax2.plot([240, 400], [160, 160], \"--k\")\n",
    "        ax2.plot([58.33333, 400], [58.33333 * 6 / 5 + 20, 400 * 6 / 5 + 20], \"--k\")\n",
    "        ax2.plot([0, 58.33333], [90, 90], \"--k\")\n",
    "        ax2.plot([0, 70], [200, 200], \"--k\")\n",
    "        ax2.plot([70, 400], [70 * 22 / 17 + 109.412, 400 * 22 / 17 + 109.412], \"--k\")\n",
    "\n",
    "        ax2.text(38, 12, \"A\")\n",
    "        ax2.text(12, 38, \"A\")\n",
    "        ax2.text(375, 240, \"B\")\n",
    "        ax2.text(260, 375, \"B\")\n",
    "        ax2.text(150, 375, \"C\")\n",
    "        ax2.text(165, 25, \"C\")\n",
    "        ax2.text(25, 125, \"D\")\n",
    "        ax2.text(375, 125, \"D\")\n",
    "        ax2.text(375, 25, \"E\")\n",
    "        ax2.text(25, 375, \"E\")\n",
    "\n",
    "        ax2.set_xlim(0, 400)\n",
    "        ax2.set_ylim(0, 400)\n",
    "\n",
    "        ax2.set_xlabel(\"True glucose value [mg/dL]\")\n",
    "        ax2.set_ylabel(\"Predicted glucose value [mg/dL]\")\n",
    "        ax2.set_title(\"Point-Error Grid Analysis\")\n",
    "\n",
    "        # P-EGA data\n",
    "        ax2.plot(ap[\"y_true\"], ap[\"y_pred\"], label=\"AP\", marker=\"o\", mec=\"xkcd:green\", mfc=\"xkcd:green\", ls=\"\")\n",
    "        ax2.plot(be[\"y_true\"], be[\"y_pred\"], label=\"BE\", marker=\"o\", mec=\"xkcd:orange\", mfc=\"xkcd:orange\", ls=\"\")\n",
    "        ax2.plot(ep[\"y_true\"], ep[\"y_pred\"], label=\"EP\", marker=\"o\", mec=\"xkcd:red\", mfc=\"xkcd:red\", ls=\"\")\n",
    "\n",
    "        ax2.legend()\n",
    "\n",
    "        # R-EGA structure\n",
    "        ax3.plot([-4, 4], [-4, 4], \"-k\", linewidth=0.75)\n",
    "        ax3.plot([-4, -1, -1], [1, 1, 4], \"-k\")\n",
    "        ax3.plot([-4, -3, 1, 1], [-1, -1, 3, 4], \"-k\")\n",
    "        ax3.plot([-4, -2, 1, 2], [-2, -1, 2, 4], \"-k\")\n",
    "        ax3.plot([-2, -1, 2, 4], [-4, -2, 1, 2], \"-k\")\n",
    "        ax3.plot([-1, -1, 3, 4], [-4, -3, 1, 1], \"-k\")\n",
    "        ax3.plot([1, 1, 4], [-4, -1, -1], \"-k\")\n",
    "\n",
    "        ax3.text(-3.25, -3.75, \"A\")\n",
    "        ax3.text(-3.75, -3.25, \"A\")\n",
    "        ax3.text(-1.35, -3.5, \"B\")\n",
    "        ax3.text(-3.5, -1.35, \"B\")\n",
    "        ax3.text(0, -3.5, \"C\")\n",
    "        ax3.text(0, 3.5, \"C\")\n",
    "        ax3.text(-3.5, 0.5, \"D\")\n",
    "        ax3.text(3.5, -0.5, \"D\")\n",
    "        ax3.text(-3.5, 3.5, \"E\")\n",
    "        ax3.text(3.5, -3.5, \"E\")\n",
    "\n",
    "        ax3.set_xlim(-4, 4)\n",
    "        ax3.set_ylim(-4, 4)\n",
    "        ax3.set_xlabel(\"True glucose rate of change [mg/dL/min]\")\n",
    "        ax3.set_ylabel(\"Predicted glucose rate of change [mg/dL/min]\")\n",
    "        ax3.set_title(\"Rate-Error Grid Analysis\")\n",
    "\n",
    "        # R-EGA data\n",
    "        ax3.plot(ap[\"dy_true\"], ap[\"dy_pred\"], label=\"AP\", marker=\"o\", mec=\"xkcd:green\", mfc=\"xkcd:green\", ls=\"\")\n",
    "        ax3.plot(be[\"dy_true\"], be[\"dy_pred\"], label=\"BE\", marker=\"o\", mec=\"xkcd:orange\", mfc=\"xkcd:orange\", ls=\"\")\n",
    "        ax3.plot(ep[\"dy_true\"], ep[\"dy_pred\"], label=\"EP\", marker=\"o\", mec=\"xkcd:red\", mfc=\"xkcd:red\", ls=\"\")\n",
    "\n",
    "        ax3.legend()\n",
    "        plt.savefig('Exp3_part1_cgega_584.pdf')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PX3ddb2VPToE"
   },
   "source": [
    "###CG EGA Code Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "s4WM1Y7NUEER",
    "outputId": "fcd187f3-7c01-4952-bf48-e0bedcaaf6a5"
   },
   "outputs": [],
   "source": [
    "freq = 5\n",
    "results = pd.DataFrame(data = np.c_[y_test.reshape(-1,1),y_pred.reshape(-1,1)], columns=[\"y_true\",\"y_pred\"])\n",
    "cg_ega = CG_EGA(results, freq)\n",
    "print(\"AP, BE, EP:\", cg_ega.reduced())\n",
    "cg_ega.plot(day=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "fIO6iyObTcrk",
    "bQtXn7iHPLlg",
    "PX3ddb2VPToE"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
